# üê¶ Twitter News Classifier v4.0

## üéØ What Is This?

The **Twitter News Classifier** is an advanced AI-powered system designed to automatically analyze, classify, and score cryptocurrency and blockchain news content from trusted Twitter accounts. Using 12 specialized AI agents working in collaboration, it provides comprehensive insights into the quality, accuracy, and relevance of social media news content.

### üöÄ Key Features

- **ü§ñ 12 Specialized AI Agents**: Each agent focuses on a specific aspect of content analysis (fact-checking, relevance, depth, etc.)
- **üìä Weighted Scoring System**: Prioritizes accuracy (18%) and relevance (15%) for reliable news classification
- **üê¶ Real Twitter API Integration**: Extracts live data from 48 trusted crypto/blockchain accounts
- **üßµ Advanced Thread Analysis**: Captures complete conversation context and related tweets
- **üñºÔ∏è Rich Media Analysis**: Analyzes images, external links, and multimedia content
- **üìà Professional Reporting**: Generates both technical (JSON) and human-readable (Markdown) reports
- **üèóÔ∏è Domain-Driven Design**: Clean, maintainable architecture with proper separation of concerns

## üéØ Who Is This For?

- **üìä Crypto Investors**: Stay informed about market-moving news with quality scoring
- **üî¨ Researchers**: Analyze social media trends and sentiment in crypto/blockchain space
- **üì∞ Content Creators**: Find high-quality, verified information for content creation
- **üíº Financial Analysts**: Get comprehensive data about social media news impact
- **üèõÔ∏è Institutions**: Monitor trusted sources for investment and research decisions

## üöÄ Quick Start

### Prerequisites

- Python 3.8 or higher
- Twitter API v2 access (Essential tier or higher)
- OpenAI API access with GPT-4 capabilities
- 8GB+ RAM for optimal performance

### Installation

1. **Clone the repository**:
```bash
git clone https://github.com/your-username/twitter-news-classifier.git
cd twitter-news-classifier
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Configure environment variables**:
Create a `.env` file in the project root:

```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-proj-your-openai-api-key

# Twitter API Configuration
TWITTER_BEARER_TOKEN=your-twitter-bearer-token
TWITTER_API_KEY=your-twitter-consumer-key
TWITTER_API_SECRET=your-twitter-consumer-secret
TWITTER_ACCESS_TOKEN=your-twitter-access-token
TWITTER_ACCESS_TOKEN_SECRET=your-twitter-access-token-secret

# Optional Configuration
MAX_TWEETS=30
HOURS_BACK=24
ENABLE_THREAD_ANALYSIS=true
ENABLE_MEDIA_ANALYSIS=true
```

4. **Run the analysis**:
```bash
python main.py
```

## üèóÔ∏è System Architecture

### Domain-Driven Design Structure

The system follows clean DDD principles with four distinct layers:

```
üìÅ Project Structure:
‚îú‚îÄ‚îÄ main.py                          # üöÄ Application entry point
‚îú‚îÄ‚îÄ domain/                         # üß† Core business logic
‚îÇ   ‚îú‚îÄ‚îÄ entities/                  # üìä Business entities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tweet.py              # üê¶ Tweet domain model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ analysis_result.py    # üìã Analysis result model
‚îÇ   ‚îî‚îÄ‚îÄ services/                 # ‚öôÔ∏è Domain services
‚îÇ       ‚îî‚îÄ‚îÄ multi_agent_analyzer.py # ü§ñ Multi-agent orchestration
‚îú‚îÄ‚îÄ application/                   # üéØ Application layer
‚îÇ   ‚îî‚îÄ‚îÄ use_cases/               # üìã Business use cases
‚îÇ       ‚îî‚îÄ‚îÄ analyze_tweets_use_case.py # üîÑ Main workflow
‚îú‚îÄ‚îÄ infrastructure/              # üîß Infrastructure layer
‚îÇ   ‚îú‚îÄ‚îÄ adapters/               # üîå External service adapters
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ twitter_api_adapter.py # üê¶ Twitter API integration
‚îÇ   ‚îú‚îÄ‚îÄ repositories/           # üíæ Data persistence
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_repository.py  # üìÅ File-based storage
‚îÇ   ‚îî‚îÄ‚îÄ prompts/               # üìù AI agent prompts
‚îÇ       ‚îî‚îÄ‚îÄ agent_prompts.py   # ü§ñ Centralized prompt repository
‚îî‚îÄ‚îÄ results/                   # üìä Analysis output storage
    ‚îî‚îÄ‚îÄ runs/                 # üìÖ Individual analysis runs
```

## ü§ñ The 12 AI Agents

### Scoring Weight Distribution

Our multi-agent system uses a carefully balanced scoring approach:

| Agent | Weight | Purpose |
|-------|--------|---------|
| **Fact Checker** | 18% | Accuracy verification and misinformation detection |
| **Context Evaluator** | 15% | Content richness and completeness assessment |
| **Relevance Analyzer** | 15% | Real-world importance and impact evaluation |
| **Depth Analyzer** | 12% | Content complexity and analytical depth |
| **Summary Agent** | 10% | Title generation and content summarization |
| **Structure Analyzer** | 8% | Organization and presentation quality |
| **Reflective Agent** | 7% | Critical thinking and bias detection |
| **Metadata Ranking** | 6% | Source credibility and authority assessment |
| **Input Preprocessor** | 5% | Data quality and normalization |
| **Consensus Agent** | 4% | Agreement and controversy evaluation |

### Agent Specializations

**üîç Core Analysis Agents**:
- **Summary Agent**: Creates compelling titles and comprehensive abstracts
- **Fact Checker**: Verifies claims and identifies potential misinformation
- **Context Evaluator**: Assesses information completeness and context richness
- **Relevance Analyzer**: Evaluates real-world importance and practical impact

**üìä Quality Assessment Agents**:
- **Depth Analyzer**: Measures content complexity and analytical thoroughness
- **Structure Analyzer**: Evaluates organization and presentation clarity
- **Reflective Agent**: Provides critical thinking and alternative perspectives
- **Consensus Agent**: Identifies controversial topics and agreement levels

**üîß Technical Agents**:
- **Input Preprocessor**: Cleans and normalizes data for better analysis
- **Metadata Ranking**: Assesses source credibility and author authority
- **Score Consolidator**: Combines individual scores using weighted formula
- **Validator**: Performs final quality assurance and error checking

## üìä Analysis Process

### Step-by-Step Workflow

1. **üê¶ Data Extraction (5-10 minutes)**
   - Connects to Twitter API v2
   - Monitors 48 trusted crypto/blockchain accounts
   - Extracts tweets with rich metadata, media, and thread context
   - Smart rate limiting to respect API constraints

2. **üîß Content Preparation (1-2 minutes)**
   - Normalizes and cleans extracted data
   - Builds comprehensive context packages
   - Prepares structured input for AI agents

3. **ü§ñ Multi-Agent Analysis (15-20 minutes)**
   - Sequential processing through 12 specialized agents
   - Each agent provides detailed scoring and reasoning
   - Comprehensive evaluation from multiple perspectives

4. **üìä Score Consolidation (2-3 minutes)**
   - Weighted combination of individual agent scores
   - Final classification and quality assessment
   - Confidence interval calculation

5. **üìã Report Generation (1-2 minutes)**
   - Individual tweet analysis reports (JSON + Markdown)
   - Executive summary with key findings
   - Organized storage in timestamped directories

### Trusted Account Monitoring

The system monitors **48 carefully curated accounts** representing:

- **Protocol Foundations**: @ethereum, @solana, @chainlink, @cardano
- **DeFi Platforms**: @Uniswap, @aave, @SushiSwap, @PancakeSwap  
- **Infrastructure**: @Polygon, @arbitrum, @Optimism, @NEARProtocol
- **Analytics**: @dune, @defipulse, @coinmetrics
- **And 36 additional trusted crypto/blockchain sources**

## ‚öôÔ∏è Configuration

### Analysis Parameters

Adjust parameters in the `AnalysisConfig` within `main.py`:
```python
config = AnalysisConfig(
    max_tweets=30,              # Number of tweets to analyze
    hours_back=24,              # Time range (hours)
    enable_thread_analysis=True, # Thread detection
    enable_media_analysis=True,  # Media content analysis
    save_individual_results=True # Individual reports
)
```

### API Configuration

**Twitter API v2 Requirements**:
- Essential tier or higher
- Full OAuth 2.0 credentials
- Rate limits: 300 user lookups, 75 timeline requests per 15 minutes

**OpenAI API Requirements**:
- GPT-4 model access (recommended) or GPT-3.5-turbo (fallback)
- Estimated usage: ~720,000 tokens per full analysis run
- Cost estimation: ~$43 per run (GPT-4) or ~$0.80 per run (GPT-3.5-turbo)

### Logging & Monitoring

Comprehensive logging is saved to `logs/` directory with detailed execution information including:
- Real-time processing status
- API rate limiting information
- Individual agent performance metrics
- Error handling and recovery actions

## üõ†Ô∏è Development & Customization

### Adding New Agents
1. Create agent prompt in `infrastructure/prompts/agent_prompts.py`
2. Add agent to the sequence in `domain/services/multi_agent_analyzer.py`
3. Configure weight in the scoring system

### Extending Analysis
- **New Data Structures**: Modify entities in `domain/entities/`
- **New Workflows**: Add use cases in `application/use_cases/`
- **External Services**: Create adapters in `infrastructure/adapters/`

### Custom Account Lists
Edit the `TRUSTED_ACCOUNTS` list in `main.py` to monitor different Twitter accounts or expand to new crypto/blockchain verticals.

## üìà Performance & Monitoring

### System Performance
- **Processing Speed**: ~30 seconds per tweet (12 agents)
- **Total Analysis Time**: 20-35 minutes for 30 tweets
- **Success Rate**: >95% under normal conditions
- **Memory Usage**: ~500MB peak during processing
- **Storage**: ~50KB per analyzed tweet

### Cost Management
- **Development**: Use GPT-3.5-turbo for testing (~$0.80 per run)
- **Production**: GPT-4 for maximum accuracy (~$43 per run)
- **Optimization**: Adjust `max_tweets` parameter for budget control

## üîí Security & Privacy

### Data Protection
- **API Key Management**: Secure environment variable configuration
- **Rate Limiting**: Built-in Twitter API rate limit handling and respect
- **Error Isolation**: Individual agent failures don't compromise entire analysis
- **No Persistent Data**: Sensitive information is not stored long-term

### Privacy Considerations
- Only analyzes publicly available Twitter content
- No personal data storage beyond public tweet metadata
- Respects Twitter's Terms of Service and API usage policies

## üìã Troubleshooting

### Common Issues

1. **Twitter API Errors**
   - ‚úÖ Verify API credentials in `.env` file
   - ‚úÖ Check API plan limits (Essential or higher required)
   - ‚úÖ Ensure monitored accounts have recent activity

2. **OpenAI API Errors**
   - ‚úÖ Verify API key validity and format
   - ‚úÖ Check usage limits and billing status
   - ‚úÖ Monitor rate limits during peak usage

3. **Performance Issues**
   - ‚úÖ Install all requirements: `pip install -r requirements.txt`
   - ‚úÖ Check Python version (3.8+ required)
   - ‚úÖ Ensure adequate RAM (8GB+ recommended)

4. **Analysis Quality**
   - ‚úÖ Use GPT-4 for best results
   - ‚úÖ Verify account list contains active accounts
   - ‚úÖ Check time range settings (24 hours default)

### Debug Information
- Check `logs/` directory for detailed execution logs
- Review individual agent outputs in results for debugging
- Monitor rate limiting messages for API optimization

## üåü Advanced Features

### Thread Analysis
- **Complete Context**: Extracts full conversation threads
- **Relationship Mapping**: Identifies reply chains and mentions
- **Context Preservation**: Maintains chronological order and relationships

### Media Content Analysis
- **Image Processing**: OCR and visual content analysis
- **Link Analysis**: External URL content extraction and evaluation
- **Rich Media Support**: Videos, documents, and multimedia content

### Quality Assurance
- **Multi-layer Validation**: Score consolidator and validator agents
- **Consistency Checking**: Cross-agent validation and error detection
- **Confidence Scoring**: Uncertainty quantification and reliability metrics

## ü§ù Contributing

We welcome contributions to improve the Twitter News Classifier:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/new-agent`
3. **Make your changes** with proper documentation
4. **Add tests** for new functionality
5. **Submit a pull request** with detailed description

### Development Guidelines
- Follow DDD principles and existing architecture
- Add comprehensive docstrings and comments
- Include unit tests for new components
- Update documentation for API changes

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üéØ Roadmap

### Upcoming Features
- **Real-time Stream Processing**: Live tweet analysis as they arrive
- **Sentiment Analysis Integration**: Advanced emotional sentiment scoring
- **Multi-language Support**: Analysis of non-English crypto content
- **API Service Mode**: REST API for integration with other systems
- **Custom Agent Framework**: User-defined analysis agents
- **Historical Trend Analysis**: Long-term pattern recognition and insights

---

**Twitter News Classifier v4.0** - Powered by AI collaboration for deep crypto/blockchain news insights.