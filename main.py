#!/usr/bin/env python3
"""
ğŸ¦ TWITTER NEWS CLASSIFIER - SEPARATED WORKFLOW
===============================================
Main entry point for the separated two-phase Twitter analysis workflow.

This implementation provides:
- Phase 1: Comprehensive tweet extraction with metadata
- Phase 2: Multi-agent analysis of extracted data
- Robust error handling between phases
- Data preservation and recovery capabilities
- Independent processing with clean separation

Usage:
    python3 main_separated_workflow.py

Features:
- Two-phase processing for maximum reliability
- API failure recovery without data loss
- Comprehensive error handling and reporting
- Clean workflow management and orchestration
"""

import os
import sys
import logging
import asyncio
from datetime import datetime
from dotenv import load_dotenv

from application.orchestrators.twitter_analysis_orchestrator import (
    TwitterAnalysisOrchestrator, 
    WorkflowConfig
)


# Updated trusted cryptocurrency and blockchain Twitter accounts list
TRUSTED_CRYPTO_ACCOUNTS = [
    "BetMode_Barry", "PendleIntern", "1inchDAO", "PolkadotInsider", "ASI_Alliance",
    "insider_sonic", "kaspaunchained", "ton_blockchain", "dydxfoundation", "0xPolygonEco",
    "pendle_fi", "GMX_IO", "algodevs", "arbitrum", "katana",
    "LidoFinance", "PancakeSwap", "SushiSwap", "Api3DAO", "0xPolygonFdn",
    "PolkadotDevs", "1inch", "fraxfinance", "AlgoFoundation", "avax",
    "Optimism", "veloprotocol", "NEARProtocol", "cosmoshub", "BandProtocol",
    "Uniswap", "SonicLabs", "solana", "Algorand", "0xPolygon",
    "Fetch_ai", "dYdX", "rendernetwork", "Rocket_Pool", "aave",
    "Cardano", "quant_network", "Cardano_CF", "StellarOrg", "ethereum",
    "Polkadot", "chainlink", "GamingOnAvax", "Ripple", "cosmos"
]


def setup_logging() -> logging.Logger:
    """Setup comprehensive logging configuration"""
    
    # Create logs directory
    os.makedirs("logs", exist_ok=True)
    
    # Configure logging
    log_filename = f"logs/twitter_workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“‹ Logging configured - Log file: {log_filename}")
    
    return logger


def verify_environment_variables() -> bool:
    """Verify all required environment variables are set"""
    
    required_vars = [
        'TWITTER_API_KEY',
        'TWITTER_API_SECRET', 
        'TWITTER_ACCESS_TOKEN',
        'TWITTER_ACCESS_TOKEN_SECRET',
        'TWITTER_BEARER_TOKEN',
        'OPENAI_API_KEY'
    ]
    
    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print("âŒ ERROR: Missing required environment variables:")
        for var in missing_vars:
            print(f"   - {var}")
        print("\nğŸ’¡ Please check your .env file and ensure all required variables are set.")
        print("   Refer to env_template.txt for the required format.")
        return False
    
    print("âœ… All required environment variables found")
    return True


def print_application_banner():
    """Print application startup banner"""
    
    banner = """
ğŸ¦ TWITTER NEWS CLASSIFIER - SEPARATED WORKFLOW v4.0
====================================================

ğŸ¯ Two-Phase Processing Architecture:
   ğŸ“¡ Phase 1: Comprehensive Tweet Extraction
   ğŸ¤– Phase 2: Multi-Agent Analysis Pipeline

ğŸ”§ Enhanced Features:
   âœ… Independent phase processing
   âœ… Robust error handling and recovery
   âœ… Data preservation between phases
   âœ… API failure tolerance
   âœ… Comprehensive workflow management

ğŸ¨ Architecture:
   ğŸ—ï¸ Domain-Driven Design (DDD)
   ğŸ“Š 12 Specialized AI Agents
   ğŸ”„ Asynchronous Processing
   ğŸ“ˆ Weighted Scoring System

Starting comprehensive analysis of crypto/blockchain Twitter content...
"""
    
    print(banner)


async def main():
    """Main execution function"""
    
    # Setup logging
    logger = setup_logging()
    
    try:
        # Print application banner
        print_application_banner()
        
        # Load environment variables
        load_dotenv()
        
        # Verify environment setup
        if not verify_environment_variables():
            sys.exit(1)
        
        logger.info("ğŸš€ Starting Twitter News Classifier - Separated Workflow")
        logger.info(f"ğŸ“Š Target accounts: {len(TRUSTED_CRYPTO_ACCOUNTS)} crypto/blockchain accounts")
        
        # Initialize orchestrator with API credentials
        orchestrator = TwitterAnalysisOrchestrator(
            twitter_api_key=os.getenv('TWITTER_API_KEY'),
            twitter_api_secret=os.getenv('TWITTER_API_SECRET'),
            twitter_access_token=os.getenv('TWITTER_ACCESS_TOKEN'),
            twitter_access_token_secret=os.getenv('TWITTER_ACCESS_TOKEN_SECRET'),
            twitter_bearer_token=os.getenv('TWITTER_BEARER_TOKEN'),
            openai_api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Configure workflow
        workflow_config = WorkflowConfig(
            max_tweets=30,
            hours_back=24,
            trusted_accounts=TRUSTED_CRYPTO_ACCOUNTS,
            max_retries=3,
            retry_delay=30,
            batch_size=5,
            continue_on_api_failure=True,
            output_base_directory="data/workflow_runs",
            cleanup_old_runs=True,
            max_old_runs_to_keep=5,
            enable_extraction_recovery=True,
            enable_analysis_recovery=True,
            save_intermediate_results=True
        )
        
        logger.info("ğŸ¯ Workflow Configuration:")
        logger.info(f"   ğŸ“Š Max tweets: {workflow_config.max_tweets}")
        logger.info(f"   â° Time window: {workflow_config.hours_back} hours")
        logger.info(f"   ğŸ‘¥ Accounts: {len(workflow_config.trusted_accounts)}")
        logger.info(f"   ğŸ”„ Max retries: {workflow_config.max_retries}")
        logger.info(f"   âš¡ Batch size: {workflow_config.batch_size}")
        logger.info(f"   ğŸ›¡ï¸ Error recovery: {workflow_config.continue_on_api_failure}")
        
        # Execute complete workflow
        logger.info("ğŸš€ Executing complete two-phase workflow...")
        
        result = await orchestrator.execute_complete_workflow(workflow_config)
        
        # Print final results
        print("\n" + "=" * 60)
        print("ğŸ“Š WORKFLOW EXECUTION COMPLETED")
        print("=" * 60)
        print(f"ğŸ†” Workflow ID: {result.workflow_id}")
        print(f"â±ï¸  Total Processing Time: {result.total_processing_time:.2f} seconds")
        print(f"ğŸ“ˆ Overall Status: {result.overall_status.upper()}")
        print(f"ğŸ Phases Completed: {result.phase_completed}")
        print(f"ğŸ’¾ Data Preserved: {'YES' if result.data_preserved else 'NO'}")
        
        if result.extraction_result:
            print(f"ğŸ“¡ Tweets Extracted: {result.extraction_result.total_tweets_extracted}")
            if result.extraction_result.tweets_file_path:
                print(f"ğŸ“‚ Extraction Data: {result.extraction_result.tweets_file_path}")
        
        if result.analysis_result:
            print(f"ğŸ¤– Successful Analyses: {result.analysis_result.successful_analyses}")
            print(f"âŒ Failed Analyses: {result.analysis_result.failed_analyses}")
            if result.analysis_result.results_file_path:
                print(f"ğŸ“‚ Analysis Results: {result.analysis_result.results_file_path}")
        
        if result.error_summary:
            print(f"âš ï¸ Total Errors: {len(result.error_summary)}")
        
        print("=" * 60)
        
        # Print recommendations
        if result.recommendations:
            print("ğŸ’¡ RECOMMENDATIONS:")
            for rec in result.recommendations:
                print(f"   {rec}")
            print("=" * 60)
        
        # Print workflow statistics
        stats = orchestrator.get_workflow_stats()
        print("ğŸ“ˆ WORKFLOW STATISTICS:")
        print(f"   ğŸ”„ Total workflows executed: {stats['workflows_executed']}")
        print(f"   âœ… Successful extractions: {stats['successful_extractions']}")
        print(f"   ğŸ¤– Successful analyses: {stats['successful_analyses']}")
        print(f"   ğŸ¦ Total tweets processed: {stats['total_tweets_processed']}")
        print(f"   âŒ Total errors: {stats['total_errors']}")
        print("=" * 60)
        
        # Log success
        if result.overall_status == 'success':
            logger.info("ğŸ‰ Complete workflow executed successfully!")
        elif result.overall_status == 'analysis_failed':
            logger.warning("âš ï¸ Extraction successful but analysis failed - data preserved for retry")
        else:
            logger.error("âŒ Workflow failed - check logs for details")
        
        return 0 if result.overall_status in ['success', 'analysis_failed'] else 1
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ Workflow interrupted by user")
        print("\nâ¹ï¸ Workflow interrupted by user")
        return 1
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Fatal error: {str(e)}", exc_info=True)
        print(f"\nğŸ’¥ Fatal error: {str(e)}")
        print("Check the log file for detailed error information.")
        return 1


if __name__ == "__main__":
    print("ğŸ¦ Twitter News Classifier - Separated Workflow")
    print("=" * 50)
    
    try:
        # Run the async main function
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Application interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Application failed to start: {str(e)}")
        sys.exit(1) 